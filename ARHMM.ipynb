{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import ssm\n",
    "import itertools\n",
    "\n",
    "import autograd.numpy          as np\n",
    "import pandas                  as pd\n",
    "\n",
    "import matplotlib.pyplot       as plt\n",
    "import seaborn                 as sns\n",
    "\n",
    "from   sklearn.model_selection import KFold\n",
    "from   sklearn.mixture         import GaussianMixture\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoregressive Hidden Markov Models for modelling baby movement\n",
    "Using the [ssm](https://github.com/lindermanlab/ssm) package.  \n",
    "\n",
    "### Aims\n",
    "- Fit (AR)HMM models to pre-processed movement data  \n",
    "- Compare model fit when including autoregressive terms in observations  \n",
    "- Use cross-validation to determine number of hidden states and lag\n",
    "\n",
    "Using Hidden Markov Models (HMM), a continuous, $D$-dimension $\\times$ $t$-timestep timeseries, $X$, can be modelled as a Markov process, $Z$, with a set of $K$ hidden states, {$z_1$, $z_2$, $...$, $z_k$}, that are not directly observable. Each state is associated with a $D$-dimensional mean $\\mu_k$ and $D \\times D$ covariance matrix $S_k$ from which the multivariate observations are drawn at each time step. The hidden states are assumed to progress as a Markov process, where the the next state is dependent only on the current state: $p(z_t | z_{t-1})$. Switches between states are governed by a $K \\times K$ transition matrix, $T$ containing state transition probabilities, $\\phi$, where $\\phi_{zz'}$ indicates the probability of transitioning from state $z$ at time $t$ to state $z'$ at time $t+1$. As any given state is only dependent on the previous state, HMMs do not capture longer-term correlations in timeseries data.\n",
    "\n",
    "In an AutoRegressive HMM (ARHMM), the observations, $x_t$ at time $t$ are dependent on $both$ the hidden state, $z_t$, and the observations at previous timespoints, {$x_{t-1}$, $x_{t-2}$, $...$, $x_{t-n}$} with $n$ determined by the degree of lag specified. As such the observation model for a given state, $z_k$, is defined as:\n",
    "\n",
    "$$ x_t | x_{t-1:L}, z_k \\sim N(\\sum_{l=1}^LA_k^{(l)}x_{t-l}+b_k,S_k)$$   \n",
    "\n",
    ".... i think this is the right formulation for multiple lags...\n",
    "\n",
    "where $x_t$ is a set of observations at time $t$, $A_k^{(l)}$ is a matrix containing the linear dynamics for the given state (the relationship between observations at time $t$ and at a given lag, $l$), $b_k$ is a state-dependent bias (average value of each variable in each state) and $S_k$ is a state-dependent diagonal covariance function where $S_{i}=0$ when $i \\neq j$, modelling observation noise.\n",
    "\n",
    "<img src=\"graphical_model.png\" width=\"500\"/>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV = 5 # number of cross-validation folds\n",
    "\n",
    "# ARHMM\n",
    "ARHMM_OBS = ['diagonal_robust_ar']\n",
    "ARHMM_K = [1, 2, 5, 10, 15, 25] #k=1 is standard AR\n",
    "ARHMM_LAGS = [1, 2, 5, 10, 15]\n",
    "ARHMM_PARAMS = list(itertools.product(ARHMM_OBS, ARHMM_K, ARHMM_LAGS))\n",
    "\n",
    "# HMM (no autoregression)\n",
    "HMM_OBS = ['diagonal_gaussian']\n",
    "HMM_K = [1, 2, 5, 10, 15, 25] \n",
    "HMM_PARAMS = list(itertools.product(HMM_OBS, HMM_K))\n",
    "\n",
    "ALL_HMM_PARAMS = HMM_PARAMS + ARHMM_PARAMS\n",
    "\n",
    "# STICKY ARHMM\n",
    "ARHMM_K = [5, 10, 15, 25] \n",
    "ARHMM_LAGS = [2, 5, 10]\n",
    "ARHMM_KAPPA = [100]\n",
    "ARHMM_TRANS = ['sticky']\n",
    "\n",
    "STICKY_ARHMM_PARAMS = list(itertools.product(ARHMM_OBS, ARHMM_K, ARHMM_LAGS, ARHMM_TRANS, ARHMM_KAPPA))\n",
    "\n",
    "# GMM (no state progression)\n",
    "GMM_OBS = ['diag']\n",
    "GMM_K = [1, 2, 5, 10, 15, 25]\n",
    "\n",
    "GMM_PARAMS = list(itertools.product(GMM_OBS, GMM_K))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load preprocessed data\n",
    "\n",
    "Data represents a $subject \\times feature \\times timepoint$ array. Where $features$ are a combination of postural and dynamic features extracted from keypoint trajectory data. Subect data are split into cross-validation folds, ensuring no subject has video in both test and train in any given fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape:  (486, 38, 4500)\n"
     ]
    }
   ],
   "source": [
    "data = np.load('outputs/processed_timeseries_data.npy')\n",
    "info = pd.read_csv('outputs/all_subject_info.csv')\n",
    "\n",
    "# take top half\n",
    "#data = data[:150]\n",
    "#info = info.iloc[:150]\n",
    "\n",
    "print('data shape: ', np.shape(data))\n",
    "num_vids, num_features, num_timepoints = data.shape\n",
    "\n",
    "\n",
    "#train test \n",
    "# split on participant (not video) to ensure that same subject data are not in both train and test\n",
    "unique_participants = info.drop_duplicates(subset = 'idnum', keep = 'first')\n",
    "X = unique_participants['idnum'].values\n",
    "# get splits\n",
    "kf = KFold(n_splits=CV, shuffle=True, random_state=1001).split(X)\n",
    "# get participant train and test ids for each fold\n",
    "kfold_idx = [(X[train_idx], X[test_idx]) for train_idx, test_idx in kf]\n",
    "# get index in 'info' and 'data' for entries that match the unique participant id\n",
    "kfold_idx = [(np.where(info['idnum'].isin(trainidx))[0], np.where(info['idnum'].isin(testidx))[0])  for trainidx, testidx in kfold_idx]\n",
    "# check there are no participants in both train and test in a given fold\n",
    "for k in np.arange(CV):\n",
    "    assert len(set(info.iloc[kfold_idx[k][0]].participant.unique()) & set(info.iloc[kfold_idx[k][1]].participant.unique())) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run cross-validation\n",
    "HMMs for each parameter set are run in a k-fold cross-valisation with per-observation average log likelihood in both train and test data as a measure of model fit. The score reflect the likelihood that a given $D$-dimensional observation could have been generated by a given model. We compare ARHMM at different lags and with different states with standard HMM (no autocorrelation) over a number of states and Gaussian Mixture Models (stationary model where observations are simply drawn from separate distributions and we do not model state progression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 1----------------------------------------------\n",
      "fitting diagonal_gaussian model with k=1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8302a366f9c6447cae9c3f77df78bfbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training LL: -53.92      testing LL: -53.85\n",
      "\n",
      "-----------------------------------------------\n",
      "fitting diagonal_gaussian model with k=2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "536d3a8c4f8c45658d4a7f08ee3c4f0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training LL: -53.09      testing LL: -53.03\n",
      "\n",
      "-----------------------------------------------\n",
      "fitting diagonal_gaussian model with k=5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c925d54ec7fc4921adbe9ab0688027cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training LL: -52.19      testing LL: -52.19\n",
      "\n",
      "-----------------------------------------------\n",
      "fitting diagonal_gaussian model with k=10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c7851b6e02347f08893fcd695c0c66a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training LL: -51.38      testing LL: -51.42\n",
      "\n",
      "-----------------------------------------------\n",
      "fitting diagonal_gaussian model with k=15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1786a1e9cc5f4a2e981c752a392d1e90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training LL: -50.93      testing LL: -51.06\n",
      "\n",
      "-----------------------------------------------\n",
      "fitting diagonal_gaussian model with k=25\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c636e4cf667420b9a4905d405ec1b76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training LL: -50.25      testing LL: -50.52\n",
      "\n",
      "-----------------------------------------------\n",
      "fitting diagonal_robust_ar model with k=1 and lags=1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08a39ff92b994dc7a40412ab77913a41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training LL: 89.39      testing LL: 87.80\n",
      "\n",
      "-----------------------------------------------\n",
      "fitting diagonal_robust_ar model with k=1 and lags=2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef68e6f245674932a2752b175ea59b79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training LL: 189.87      testing LL: 187.90\n",
      "\n",
      "-----------------------------------------------\n",
      "fitting diagonal_robust_ar model with k=1 and lags=5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9bfd33f1ab74716a6bd27edce3249af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training LL: 324.34      testing LL: 322.55\n",
      "\n",
      "-----------------------------------------------\n",
      "fitting diagonal_robust_ar model with k=1 and lags=10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59b2a3fe94954e5bb9af39cacc013fa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training LL: 361.65      testing LL: 360.10\n",
      "\n",
      "-----------------------------------------------\n",
      "fitting diagonal_robust_ar model with k=1 and lags=15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4240a0c0658c4d43b745793fa5ef9edf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training LL: 363.11      testing LL: 361.63\n",
      "\n",
      "-----------------------------------------------\n",
      "fitting diagonal_robust_ar model with k=2 and lags=1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07cba6da821a4a2faa46fb3fbfea22a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training LL: 91.19      testing LL: 89.54\n",
      "\n",
      "-----------------------------------------------\n",
      "fitting diagonal_robust_ar model with k=2 and lags=2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6633c901ea54fc6ab0d3bff17dc2de9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training LL: 192.82      testing LL: 190.82\n",
      "\n",
      "-----------------------------------------------\n",
      "fitting diagonal_robust_ar model with k=2 and lags=5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3122e5ef62c54cc181c585c5902d61c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training LL: 335.75      testing LL: 333.94\n",
      "\n",
      "-----------------------------------------------\n",
      "fitting diagonal_robust_ar model with k=2 and lags=10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5609a03176b14f95a5564fc607700f7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training LL: 362.45      testing LL: 360.75\n",
      "\n",
      "-----------------------------------------------\n",
      "fitting diagonal_robust_ar model with k=2 and lags=15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2f5aef1ecba45d5b45e55f13b8cb4fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training LL: 369.37      testing LL: 367.96\n",
      "\n",
      "-----------------------------------------------\n",
      "fitting diagonal_robust_ar model with k=5 and lags=1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d0f57df936649c28103fe9c43f5eed8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training LL: 93.50      testing LL: 91.76\n",
      "\n",
      "-----------------------------------------------\n",
      "fitting diagonal_robust_ar model with k=5 and lags=2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1d9d205507348b2ab0102b6bd9aca58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training LL: 195.07      testing LL: 193.03\n",
      "\n",
      "-----------------------------------------------\n",
      "fitting diagonal_robust_ar model with k=5 and lags=5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b8ad1e4dca64c4f987b37f99f0f7a59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m train_data_shuffled \u001b[38;5;241m=\u001b[39m [train_data_scaled[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m ridx]\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# model fit\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m lls \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data_shuffled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mem\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# record scores\u001b[39;00m\n\u001b[1;32m     37\u001b[0m train_score \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mlog_likelihood(train_data_scaled)\n",
      "File \u001b[0;32m~/PROJECTS/babyMoves/state-space/ssm/ssm/util.py:111\u001b[0m, in \u001b[0;36mensure_args_are_lists.<locals>.wrapper\u001b[0;34m(self, datas, inputs, masks, tags, **kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tags, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m    109\u001b[0m     tags \u001b[38;5;241m=\u001b[39m [tags]\n\u001b[0;32m--> 111\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmasks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PROJECTS/babyMoves/state-space/ssm/ssm/hmm.py:504\u001b[0m, in \u001b[0;36mHMM.fit\u001b[0;34m(self, datas, inputs, masks, tags, verbose, method, initialize, init_method, **kwargs)\u001b[0m\n\u001b[1;32m    501\u001b[0m          \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly EM is implemented for constrained transitions.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    503\u001b[0m \u001b[38;5;66;03m# print(verbose)\u001b[39;00m\n\u001b[0;32m--> 504\u001b[0m  \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_fitting_methods\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mmasks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmasks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m                                 \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PROJECTS/babyMoves/state-space/ssm/ssm/hmm.py:449\u001b[0m, in \u001b[0;36mHMM._fit_em\u001b[0;34m(self, datas, inputs, masks, tags, verbose, num_iters, tolerance, init_state_mstep_kwargs, transitions_mstep_kwargs, observations_mstep_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    445\u001b[0m pbar \u001b[38;5;241m=\u001b[39m ssm_pbar(num_iters, verbose, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLP: \u001b[39m\u001b[38;5;132;01m{:.1f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, [lls[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m itr \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;66;03m# E step: compute expected latent states with current parameters\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m     expectations \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpected_states(data, \u001b[38;5;28minput\u001b[39m, mask, tag)\n\u001b[1;32m    450\u001b[0m                     \u001b[38;5;28;01mfor\u001b[39;00m data, \u001b[38;5;28minput\u001b[39m, mask, tag,\n\u001b[1;32m    451\u001b[0m                     \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(datas, inputs, masks, tags)]\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;66;03m# M step: maximize expected log joint wrt parameters\u001b[39;00m\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_state_distn\u001b[38;5;241m.\u001b[39mm_step(expectations, datas, inputs, masks, tags, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minit_state_mstep_kwargs)\n",
      "File \u001b[0;32m~/PROJECTS/babyMoves/state-space/ssm/ssm/hmm.py:449\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    445\u001b[0m pbar \u001b[38;5;241m=\u001b[39m ssm_pbar(num_iters, verbose, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLP: \u001b[39m\u001b[38;5;132;01m{:.1f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, [lls[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m itr \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;66;03m# E step: compute expected latent states with current parameters\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m     expectations \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpected_states\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtag\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    450\u001b[0m                     \u001b[38;5;28;01mfor\u001b[39;00m data, \u001b[38;5;28minput\u001b[39m, mask, tag,\n\u001b[1;32m    451\u001b[0m                     \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(datas, inputs, masks, tags)]\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;66;03m# M step: maximize expected log joint wrt parameters\u001b[39;00m\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_state_distn\u001b[38;5;241m.\u001b[39mm_step(expectations, datas, inputs, masks, tags, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minit_state_mstep_kwargs)\n",
      "File \u001b[0;32m~/PROJECTS/babyMoves/state-space/ssm/ssm/util.py:158\u001b[0m, in \u001b[0;36mensure_args_not_none.<locals>.wrapper\u001b[0;34m(self, data, input, mask, tag, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],) \u001b[38;5;241m+\u001b[39m M) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n\u001b[1;32m    157\u001b[0m mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones_like(data, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m mask\n\u001b[0;32m--> 158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PROJECTS/babyMoves/state-space/ssm/ssm/hmm.py:258\u001b[0m, in \u001b[0;36mHMM.expected_states\u001b[0;34m(self, data, input, mask, tag)\u001b[0m\n\u001b[1;32m    256\u001b[0m Ps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransitions\u001b[38;5;241m.\u001b[39mtransition_matrices(data, \u001b[38;5;28minput\u001b[39m, mask, tag)\n\u001b[1;32m    257\u001b[0m log_likes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservations\u001b[38;5;241m.\u001b[39mlog_likelihoods(data, \u001b[38;5;28minput\u001b[39m, mask, tag)\n\u001b[0;32m--> 258\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhmm_expected_states\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpi0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_likes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PROJECTS/babyMoves/state-space/ssm/ssm/messages.py:203\u001b[0m, in \u001b[0;36mhmm_expected_states\u001b[0;34m(pi0, Ps, ll)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;66;03m# Compute the sum over time axis of the expected joints\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     expected_joints \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((K, K))\n\u001b[0;32m--> 203\u001b[0m     \u001b[43m_compute_stationary_expected_joints\u001b[49m\u001b[43m(\u001b[49m\u001b[43malphas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbetas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mll\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_Ps\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpected_joints\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m     expected_joints \u001b[38;5;241m=\u001b[39m expected_joints[\u001b[38;5;28;01mNone\u001b[39;00m, :, :]\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m expected_states, expected_joints, normalizer\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# space for results\n",
    "hmm_results = np.zeros((len(ALL_HMM_PARAMS), 4, CV)) # model x train/test scores x folds\n",
    "shmm_results = np.zeros((len(STICKY_ARHMM_PARAMS), 4, CV)) # model x train/test scores x folds\n",
    "gmm_results = np.zeros((len(GMM_PARAMS), 4, CV)) # model x train/test scores x folds\n",
    "\n",
    "for f, (train_idx, test_idx) in enumerate(kfold_idx):\n",
    "    print('FOLD {:}----------------------------------------------'.format(f+1))\n",
    "    train_data = [data[i,:,:].T for i in train_idx]\n",
    "    test_data = [data[i,:,:].T for i in test_idx]\n",
    "    \n",
    "    # scale to unit SD prior to modelling using train data to estimate\n",
    "    obs_sd = np.std(np.concatenate(train_data), 0)\n",
    "    train_data_scaled = [i / obs_sd for i in train_data]\n",
    "    test_data_scaled = [i / obs_sd for i in test_data]\n",
    "\n",
    "    # parse model parameters\n",
    "    for n_p, p_set in enumerate(ALL_HMM_PARAMS):\n",
    "        if len(p_set)==3:\n",
    "            obs, k, lag = [*p_set]\n",
    "            model = ssm.HMM(k, num_features, observations=obs, observation_kwargs={'lags':lag})\n",
    "            print('fitting {:} model with k={:} and lags={:}'.format(obs, k, lag))\n",
    "        else:\n",
    "            assert(len(p_set)==2)\n",
    "            obs, k = [*p_set]\n",
    "            lag = None\n",
    "            model = ssm.HMM(k, num_features, observations=obs)\n",
    "            print('fitting {:} model with k={:}'.format(obs, k))\n",
    "\n",
    "        # fit\n",
    "        # shuffle data before fitting\n",
    "        ridx = np.random.choice(np.arange(len(train_data_scaled)), size=len(train_data_scaled), replace=False)\n",
    "        train_data_shuffled = [train_data_scaled[i] for i in ridx]\n",
    "        # model fit\n",
    "        lls = model.fit(train_data_shuffled, method='em', num_iters=50)\n",
    "\n",
    "        # record scores\n",
    "        train_score = model.log_likelihood(train_data_scaled)\n",
    "        train_av_score = train_score / (len(train_data_scaled) * len(train_data_scaled[0]))\n",
    "        test_score = model.log_likelihood(test_data_scaled)\n",
    "        test_av_score = test_score / (len(test_data_scaled) * len(test_data_scaled[0]))\n",
    "        \n",
    "        print('training LL: {:.2f}      testing LL: {:.2f}'.format(train_av_score, test_av_score))\n",
    "        print('')\n",
    "        print('-----------------------------------------------')\n",
    "\n",
    "        # store\n",
    "        hmm_results[n_p, 0, f] = train_score\n",
    "        hmm_results[n_p, 1, f] = train_av_score\n",
    "        hmm_results[n_p, 2, f] = test_score\n",
    "        hmm_results[n_p, 3, f] = test_av_score\n",
    "        \n",
    "    for n_p, p_set in enumerate(STICKY_ARHMM_PARAMS):\n",
    "        obs, k, lag, trans, kappa = [*p_set]\n",
    "        model = ssm.HMM(k, num_features, observations=obs, observation_kwargs={'lags':lag}, transition=trans, transition_kwargs={'kappa':kappa})\n",
    "        print('fitting sticky {:} model with k={:} and lags={:}'.format(obs, k, lag))\n",
    "\n",
    "        # fit\n",
    "        # shuffle data before fitting\n",
    "        ridx = np.random.choice(np.arange(len(train_data_scaled)), size=len(train_data_scaled), replace=False)\n",
    "        train_data_shuffled = [train_data_scaled[i] for i in ridx]\n",
    "        # model fit\n",
    "        lls = model.fit(train_data_shuffled, method='em', num_iters=50)\n",
    "\n",
    "        # record scores\n",
    "        train_score = model.log_likelihood(train_data_scaled)\n",
    "        train_av_score = train_score / (len(train_data_scaled) * len(train_data_scaled[0]))\n",
    "        test_score = model.log_likelihood(test_data_scaled)\n",
    "        test_av_score = test_score / (len(test_data_scaled) * len(test_data_scaled[0]))\n",
    "        \n",
    "        print('training LL: {:.2f}      testing LL: {:.2f}'.format(train_av_score, test_av_score))\n",
    "        print('')\n",
    "        print('-----------------------------------------------')\n",
    "\n",
    "        # store\n",
    "        shmm_results[n_p, 0, f] = train_score\n",
    "        shmm_results[n_p, 1, f] = train_av_score\n",
    "        shmm_results[n_p, 2, f] = test_score\n",
    "        shmm_results[n_p, 3, f] = test_av_score    \n",
    "        \n",
    "    for n_p, p_set in enumerate(GMM_PARAMS):\n",
    "        obs, k = [*p_set]\n",
    "        \n",
    "        concat_data = np.concatenate(train_data_shuffled)\n",
    "        \n",
    "        model = GaussianMixture(n_components=k, covariance_type=obs, max_iter=50)\n",
    "        model.fit(concat_data)\n",
    "\n",
    "        train_av_score = model.score(concat_data)\n",
    "        train_score = train_av_score * len(concat_data)\n",
    "        test_av_score = model.score(np.concatenate(test_data_scaled))\n",
    "        test_score = test_score * len(np.concatenate(test_data_scaled))\n",
    "        \n",
    "        print('fitting gaussian mixture model with k={:}'.format(k))\n",
    "        print('training LL: {:.2f}      testing LL: {:.2f}'.format(train_av_score, test_av_score))\n",
    "        print('')\n",
    "        print('-----------------------------------------------')\n",
    "\n",
    "        # store\n",
    "        gmm_results[n_p, 0, f] = train_score\n",
    "        gmm_results[n_p, 1, f] = train_av_score\n",
    "        gmm_results[n_p, 2, f] = test_score\n",
    "        gmm_results[n_p, 3, f] = test_av_score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate and save out CV results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HMM\n",
    "param_df = pd.DataFrame(ALL_HMM_PARAMS)\n",
    "param_df.fillna(value=0, inplace=True)\n",
    "\n",
    "hmm_results_df = pd.DataFrame()\n",
    "for f in np.arange(CV):\n",
    "    fold_df = pd.DataFrame(pd.concat((param_df, pd.DataFrame(hmm_results[:,:,f])), axis=1))\n",
    "    fold_df.columns = ['obs_model', 'states', 'lag', 'train_LL', 'train_av_LL', 'test_LL', 'test_av_LL']\n",
    "    fold_df.insert(2, 'fold', [f+1]*len(ALL_HMM_PARAMS))\n",
    "    hmm_results_df = pd.concat((hmm_results_df, fold_df)).reset_index(drop=True)\n",
    "\n",
    "# STICKY HMM    \n",
    "param_df = pd.DataFrame(STICKY_ARHMM_PARAMS)\n",
    "param_df.fillna(value=0, inplace=True)\n",
    "\n",
    "shmm_results_df = pd.DataFrame()\n",
    "for f in np.arange(CV):\n",
    "    fold_df = pd.DataFrame(pd.concat((param_df, pd.DataFrame(shmm_results[:,:,f])), axis=1))\n",
    "    fold_df.columns = ['obs_model', 'states', 'lag', 'transition', 'kappa', 'train_LL', 'train_av_LL', 'test_LL', 'test_av_LL']\n",
    "    fold_df.insert(2, 'fold', [f+1]*len(STICKY_ARHMM_PARAMS))\n",
    "    shmm_results_df = pd.concat((shmm_results_df, fold_df)).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# GMM    \n",
    "param_df = pd.DataFrame(GMM_PARAMS)\n",
    "param_df.fillna(value=0, inplace=True)\n",
    "\n",
    "gmm_results_df = pd.DataFrame()\n",
    "for f in np.arange(CV):\n",
    "    fold_df = pd.DataFrame(pd.concat((param_df, pd.DataFrame(gmm_results[:,:,f])), axis=1))\n",
    "    fold_df.columns = ['obs_model', 'states', 'train_LL', 'train_av_LL', 'test_LL', 'test_av_LL']\n",
    "    fold_df.insert(2, 'fold', [f+1]*len(GMM_PARAMS))\n",
    "    gmm_results_df = pd.concat((gmm_results_df, fold_df)).reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "all_results = pd.concat((hmm_results_df, shmm_results_df, gmm_results_df), axis=0, ignore_index=True)\n",
    "all_results.to_csv('outputs/cross-validation-results.csv', index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
